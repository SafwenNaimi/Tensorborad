WARNING:tensorflow:From main.py:32: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
True
['ants', 'bees']
ConvNeXt(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (2): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): Identity()
      )
      (2): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): Identity()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
      (2): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
      (3): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
      (4): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
      (5): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
      (6): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
      (7): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
      (8): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): Identity()
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): Identity()
      )
      (2): Block(
        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): Identity()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=2, bias=True)
)
Epoch 0/32
----------
train: Loss: 0.7723 Acc: 0.5492
test: Loss: 0.7124 Acc: 0.5556
Epoch 1/32
----------
train: Loss: 0.7466 Acc: 0.5246
test: Loss: 0.7760 Acc: 0.5490
Epoch 2/32
----------
train: Loss: 0.7084 Acc: 0.5656
test: Loss: 0.7345 Acc: 0.5490
Epoch 3/32
----------
train: Loss: 0.6803 Acc: 0.6025
test: Loss: 0.7575 Acc: 0.5686
Epoch 4/32
----------
train: Loss: 0.6752 Acc: 0.6107
test: Loss: 0.7848 Acc: 0.5163
Epoch 5/32
----------
train: Loss: 0.7115 Acc: 0.6107
test: Loss: 0.7275 Acc: 0.5294
Epoch 6/32
----------
train: Loss: 0.6880 Acc: 0.5410
test: Loss: 0.7280 Acc: 0.5686
Epoch 7/32
----------
train: Loss: 0.6594 Acc: 0.6148
test: Loss: 0.7185 Acc: 0.5556
Epoch 8/32
----------
train: Loss: 0.6668 Acc: 0.6352
test: Loss: 0.7128 Acc: 0.5621
Epoch 9/32
----------
train: Loss: 0.6335 Acc: 0.6230
test: Loss: 0.7128 Acc: 0.5686
Epoch 10/32
----------
train: Loss: 0.6468 Acc: 0.6393
test: Loss: 0.7198 Acc: 0.5621
Epoch 11/32
----------
train: Loss: 0.6227 Acc: 0.6803
test: Loss: 0.7179 Acc: 0.5752
Epoch 12/32
----------
train: Loss: 0.6187 Acc: 0.6475
test: Loss: 0.7144 Acc: 0.5817
Epoch 13/32
----------
train: Loss: 0.6619 Acc: 0.6025
test: Loss: 0.7082 Acc: 0.5752
Epoch 14/32
----------
train: Loss: 0.6402 Acc: 0.6189
test: Loss: 0.7076 Acc: 0.5752
Epoch 15/32
----------
train: Loss: 0.6463 Acc: 0.6107
test: Loss: 0.7078 Acc: 0.5752
Epoch 16/32
----------
train: Loss: 0.6433 Acc: 0.6148
test: Loss: 0.7073 Acc: 0.5752
Epoch 17/32
----------
train: Loss: 0.6486 Acc: 0.6025
test: Loss: 0.7070 Acc: 0.5752
Epoch 18/32
----------
train: Loss: 0.6355 Acc: 0.6393
test: Loss: 0.7074 Acc: 0.5752
Epoch 19/32
----------
train: Loss: 0.6412 Acc: 0.6107
test: Loss: 0.7072 Acc: 0.5686
Epoch 20/32
----------
train: Loss: 0.6241 Acc: 0.6803
test: Loss: 0.7071 Acc: 0.5686
Epoch 21/32
----------
train: Loss: 0.6413 Acc: 0.6230
test: Loss: 0.7071 Acc: 0.5686
Epoch 22/32
----------
train: Loss: 0.6695 Acc: 0.5656
test: Loss: 0.7071 Acc: 0.5686
Epoch 23/32
----------
train: Loss: 0.6239 Acc: 0.6803
test: Loss: 0.7071 Acc: 0.5686
Epoch 24/32
----------
train: Loss: 0.6085 Acc: 0.6721
test: Loss: 0.7071 Acc: 0.5686
Epoch 25/32
----------
train: Loss: 0.6473 Acc: 0.6230
test: Loss: 0.7070 Acc: 0.5686
Epoch 26/32
----------
train: Loss: 0.6285 Acc: 0.6516
test: Loss: 0.7070 Acc: 0.5686
Epoch 27/32
----------
train: Loss: 0.6679 Acc: 0.5861
test: Loss: 0.7070 Acc: 0.5686
Epoch 28/32
----------
train: Loss: 0.6149 Acc: 0.6844
test: Loss: 0.7070 Acc: 0.5686
Epoch 29/32
----------
train: Loss: 0.6571 Acc: 0.6148
test: Loss: 0.7070 Acc: 0.5686
Epoch 30/32
----------
train: Loss: 0.6293 Acc: 0.6352
test: Loss: 0.7070 Acc: 0.5686
Epoch 31/32
----------
train: Loss: 0.6352 Acc: 0.6270
test: Loss: 0.7070 Acc: 0.5686
Epoch 32/32
----------
train: Loss: 0.6381 Acc: 0.5943
test: Loss: 0.7070 Acc: 0.5686
Training complete in 2m 10s
Best val Acc: 0.581699